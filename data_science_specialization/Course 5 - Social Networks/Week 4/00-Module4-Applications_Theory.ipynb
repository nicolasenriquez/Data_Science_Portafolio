{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preferential Attachment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distributions\n",
    "\n",
    "As we have seen before, the **degree** of a node in an undirected graph is the number of neighbors that it has, but sometimes we're not interested in that particular degree of a specific node. We're interested in seeing how the degrees of all the nodes are distributed across the whole network.\n",
    "\n",
    "The **degree distribution** of a graph is the probability distribution of the degrees over the entire network.\n",
    "\n",
    "#### Degree Distributions - Undirected Graphs\n",
    "\n",
    "We can plot the degree distribution of a network\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Estimate the degrees of the network\n",
    "degrees = G.degree()\n",
    "\n",
    "# Set of the unique values othe degrees of the network\n",
    "degrees_values = sorted(set(degrees.values()))\n",
    "\n",
    "# Histogram\n",
    "histogram = [list(degrees.values()).count(i) / float(nx.number_of_nodes(G)) for i in degrees_values]\n",
    "\n",
    "# Graph - Histogram of the degrees distribution of the network\n",
    "plt.bar(degrees_values, histogram)\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Fraction of Nodes\")\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "#### Degree Distributions - Directed Graphs\n",
    "\n",
    "Sometimes when we're working with directed graphs instead of undirected graphs, we **would be interested in the in-degree or the out-degree of the nodes, usually, we look at the in-degree**.\n",
    "\n",
    "- **In-Degree :** The number of nodes in a directed graph that shows the number of *in-links* the node has\n",
    "- **Out-Degree :** The number of nodes in a directed graph that shows the number of *out-links* the node has\n",
    "\n",
    "We can plot the *in-degree* distribution of a network\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Estimate the in-degrees of the network\n",
    "in_degrees = G.in_degree()\n",
    "\n",
    "# Set of the unique values othe degrees of the network\n",
    "in_degrees_values = sorted(set(in_degree.values()))\n",
    "\n",
    "# Histogram\n",
    "histogram = [list(in_degrees.values()).count(i) / float(nx.number_of_nodes(G)) for i in in_degrees_values]\n",
    "\n",
    "# Graph - Histogram of the degrees distribution of the network\n",
    "plt.bar(in_degrees_values, histogram)\n",
    "plt.xlabel(\"In-Degree\")\n",
    "plt.ylabel(\"Fraction of Nodes\")\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Distributions in the Real Networks\n",
    "\n",
    "Let's see three examples (3 graficos, con un scatter escalonado con tendencia negativa, mas, una \"recta de reg\" tambien con tendencia a la baja, es decir, pendiente negativa). \n",
    "\n",
    "**There are two things to notice about these degree distributions (graph example not so relevant, keep reading). The first thing is that they're all in log-log scale, meaning that the x-axis and the y-axis are both on log scale rather than linear scale. The second thing to notice is that, for at least part of these distributions, they tend to look like straight lines for all three cases. When you put those two things together when you have a degree distribution on a log-log scale and it looks kind of like a straight line, then we say that this degree distribution looks kind of like a power law** (For this example the values of alpha are $\\alpha-values = \\textit{A: 2.3, B: 2.1, C: 4.0}$), and for the more applied than theoretical (look *equation below*), the **thing to know about power law degree distributions is that they tend to have most of the nodes with very, very small degree, but you have a few nodes that accumulate a very, very large degree**.\n",
    "\n",
    "$$\\textbf{Power Law: } P(k) = Ck^{-\\alpha}$$\n",
    "$$where, \\rightarrow \\alpha, C = constants $$\n",
    "\n",
    "One of the things we try to ask when we see something like this is, what could explain this property that we observe happening in many different networks (power law degree distribution is present???)? The way we try to answer this question is by coming up with models that generate networks that make a few assumptions about how these networks get formed, and then they give rise to whatever properties we observe. So in this case, the question would be, can we come up with a model that generates a network that has a power law-like degree distribution? One of the models that achieves this property is called a Preferential Attachment Model.\n",
    "\n",
    "\n",
    "### Preferential Attachment Model\n",
    "\n",
    "Let's see how the preferential attachment model works. First, we start with just two nodes that are connected by an edge, and then at each time step, we're going to add a new node, and the new node is going to connect to a single existing node. The sort of special sauce in the model comes when you decide which node to pick out of the existing nodes. The way that these new node is going to pick an existing node to attach to is going to be that it's going to choose it at random, but it's going to choose it with probability proportional to the node's current degree. So the probability of connecting to a node $u$ that has a degree $k_{u}$, is going to be $k_{u}$ divided by the sum of all the degrees of all the other nodes.\n",
    "\n",
    "$$\\textit{Probability of Connecting} = \\frac{k_{u}}{\\sum_{j = 0}^{n}{k_{j}}}$$\n",
    "\n",
    "\n",
    "This type of modeling technique allows us to explain or at least have some hypothesis for what kind of mechanism could give and rise to this shape of the degree distribution that we observe.\n",
    "\n",
    "For example, if we believe that a very popular actor that has appeared with many other actors in movies has a higher likelihood of getting an additional actor to co-appear in a movie than a maybe less popular actor that has not appeared with many other actors in movies, then this is the kind of mechanism that could be explaining the sort of very skewed power law distribution that we observed.\n",
    "\n",
    "\n",
    "\n",
    "### Preferential Attachment Model using NetworkX\n",
    "\n",
    "In `NetworkX`, you can use the function, `barabasi_albert_graph(n, m)`, which is named after the researchers that came up with this model, with input powers n and m, where **n is the number of nodes, and m is the number of new nodes that an arriving node would attach to**.\n",
    "\n",
    "In our example, the way we define it, this m parameter would be one because we said that every new node would attach to only a single existing node, but you can generalize this and have it so that every node attaches to m existing nodes, and that m will not change the fact that you still get a power law degree distribution.\n",
    "\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the graph\n",
    "G = nx.barabasi_albert_graph(n=1000000, m=1)\n",
    "\n",
    "# Estimate the degrees of the network barabasi_albert_graph()\n",
    "degrees = G.degree()\n",
    "\n",
    "# Set of the unique values othe degrees of the network\n",
    "degrees_values = sorted(set(degree.values()))\n",
    "\n",
    "# Histogram\n",
    "histogram = [list(degrees.values()).count(i) / float(nx.number_of_nodes(G)) for i in degrees_values]\n",
    "\n",
    "# Graph - Scatter Plot of the degrees distribution of the network\n",
    "plt.plot(degrees_values, histogram)\n",
    "plt.xlabel(\"Degrees (log scale)\")\n",
    "plt.ylabel(\"Fraction of Nodes (log scale)\")\n",
    "\n",
    "# Set the x-axis and y-axis to log scale\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, we have the **degree distribution of a graph is the probability distribution of the degrees over the entire network, and in many real networks that we see, this degree distribution tends to look sort of like a power law because it looks sort of like a straight line on a log-log scale**. \n",
    "\n",
    "We also discussed that models of networks generation allows to identify mechanisms that give rise to patterns that we observe in real networks. In this case, we observed that **many real networks tend to have these power law degree distribution, and then we covered a model that gives rise to this particular property which allows us to have some insight about the kinds of things that could be given rise to these properties in real networks**. \n",
    "\n",
    "In this case, this was the Preferential Attachment Model that produces these networks with power law degree distribution. (Los datos generados a partir de este modelo poseen us distribucion *power law degree*)\n",
    "\n",
    "In `NetworkX`, you can use this function, `barabasi_albert_graph(m,n)` to construct networks that have n nodes and where every single node attaches to m existing nodes following the Preferential Attachment Model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small World Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, today we're going to talk about the small world phenomenon, which suggests that the world is small in the sense that, we're all connected by very short paths between each other.\n",
    "\n",
    "- How can we measure these paths, and how short are they really?\n",
    "\n",
    "\n",
    "\n",
    "### Property Nº1 - Average Shortest Path between Random Pairs of Nodes (avg, median tends to be small - 7)\n",
    "\n",
    "*...Story of Milgram Experiment in 1960's...*\n",
    "\n",
    "Now more recently, people have tried to answer this question but without actually running an experiment, instead looking at actual data in measuring how long these paths are. Of course in 1960s, we do not have access to very large data sets of networks but nowadays, we do. So one example of this is, looking at instant message communication among people. So researchers took a network of 240 million active users on Microsoft Instant Messenger. And then they connected them if two users were engaged in a two-way communication over a period of a month. And so these defined a network, a very large network. **And the estimated path length in this network, so if you take any two people at random and check what their distance between them in the network, so the length of the shortest path, the medium value over this huge sample is estimated to be 7**. Which is very close to what Milgram had found in the 1960s which was 6, and it's also very small.\n",
    "\n",
    "- *Nodes*: 240 million active users  on Microsoft Instant Menssenger\n",
    "\n",
    "- *Edges*: Users engaged in a two-way communication over a one-month period\n",
    "\n",
    "- Estimated Median Path Length: 7 \n",
    "\n",
    "\n",
    "So what this tells us is that, if we look at these real networks that have millions and millions of people, the average shortest path between pairs of notes tend to be on average or median, tends to be very small.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Property Nº2 -  Clustering Coefficient Property\n",
    "\n",
    "So remember the local clustering coefficient of a node, is the fraction of pairs of the nodes friends who are friends with each other. It's roughly talks about, are there lots of triangles are not?\n",
    "\n",
    "And when we look at real networks, like for example Facebook in 2011, we find that the average clustering coefficient tends to be pretty high. So **for people that have lots and lots of friends, their clustering coefficient tends to be smaller, but on average, it's still pretty large**. So if you look at nodes that have say, 20 to 50 friends, they're clustering coefficient is somewhere in the 30's or so.\n",
    "\n",
    "I say that these clustering coefficients are high because, if you imagine that these graphs were completely random. Meaning, you take all of these nodes and whatever number of edges it has and you kind of just assign them at random, then you would find that the average clustering coefficient would be much, much smaller because there is nothing that's sort of making these triangles appear, right? And so, these clustering coefficients tend to be high. We think that there is something happening in the formation of the network that is sort of favoring triangle formation.\n",
    "\n",
    "\n",
    "### Path Length and Clustering\n",
    "\n",
    "So we've noticed two things from the example just seen, one, that social networks tend to have a **high clustering coefficient** and two, they tend to have **small average path length**. And so the next question is:\n",
    "\n",
    "- Can we think of a model that generates networks that have these two properties?\n",
    "\n",
    "\n",
    "- How about the Preferencial Attachment Model?\n",
    "\n",
    "\n",
    "- Can we check if this particular model has networks with high clustering coefficient and small average path length?\n",
    "\n",
    "\n",
    "So let's create one network of a 1,000 nodes and parameter m of 4, that means that each new node is going to attach to 4 existing nodes. And let's see what its average clustering is.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graph with 1000 nodes and 4 possible connections from each node\n",
    "G = nx.barabasi_albert_graph(n=1000, m=1)\n",
    "\n",
    "# Estimate the average clustering score\n",
    "print(nx.average_clustering(G))\n",
    "Out: 0.020285\n",
    "    \n",
    "# Estimate the average clustering score\n",
    "print(nx.average_shortest_path_length(G))\n",
    "Out: 4.169429\n",
    "```\n",
    "\n",
    "Let's see what its average clustering is. Well in this case, it's 0.02 which is pretty small compared to the networks that we had seen before. Now for the average shortest path length, it is 4.16, which is pretty small, it's pretty decent. So it seems that it get's one of the properties but not the other.\n",
    "\n",
    "And so it seems like the preferential attachment model, while it has the small average shortest path property, fails to have these cluster and coefficient, high cluster and coefficient property. And the reason is that, there is no mechanism in the preferential attachment model that would favor triangle formation. So it's natural that it just doesn't have that property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small World Model\n",
    "\n",
    "\n",
    "- **Motivation :** Real Network exhibit high clustering coefficient and small average shortest paths. Can we think of a model that achieves boths of these properties?\n",
    "\n",
    "\n",
    "Let's talk about a new model called the small world model that actually gives network that have these two properties. So first, let's discuss how the model actually works:\n",
    "\n",
    "1. Starts with a ring of *n* nodes, where each node is connected to it's *k* nearest neighbors\n",
    "2. Then, fix a parameter $\\rho \\in [0,1]$ \n",
    "3. Then, considered for each edge $(u, v)$, with a probability value $p$, select a node $w$ at random and rewire the edge $(u, v)$ so it becomes $(u, w)$. This is a process that ocurrs at random, similar to a Montecarlo simulation, where the event happends with a probability $p$ and does not with a probability $(1-p)$. In this case, some edges get rewired, and some doesn't. If it's rewired, it will choose another node at random and change the edge between nodes.\n",
    "\n",
    "So let's say we're taking the edge u,v with probability p, we're going to find another node w, completely at random. Or we're going to rewire it so that the edge u,v becomes u,w. You do this for each one of the edges and you're done.\n",
    "\n",
    "- What happends when we change these *'parameter'* $p$?\n",
    "\n",
    "Probability $p$ of rewireing a node extreme scenarios:\n",
    "- **Regular Lattice** $\\rightarrow p = 0$, $ \\therefore \\textit{no edge is rewired}$ \n",
    "\n",
    "- **Random Network** $\\rightarrow p = 1$, $ \\therefore \\textit{all edge are rewired}$\n",
    "\n",
    "- **Small World Network** $\\rightarrow 0 < p < 1, \\therefore$ some edges are rewired, the Network conserves some local structure but has some randomness \n",
    "\n",
    "\n",
    "When p is 0, so we're looking at this type of network. What we have is a completely regular lattice. So there is no rewiring, and because every node is connected to k of its neighbors, then there are lots of triangles that get formed locally, right? Because, well it depends on the value of k, but if k is sort of large enough, then you start to form many triangles. And so this network will have pretty high clustering coefficient because it purposely going to form triangles in the beginning. And then nothing gets rewire, nothing gets changed, so it has a pretty high clustering coefficient. However, a disadvantage of a regular lattice with almost zero variation (imagine a flower drawn by a kid)\n",
    "f you imagine there's been a very large network, you can imagine that the distances between nodes can get very large, right. So if you're in one side of the ring to get to the other side of the ring, you can hop a little bit but there is no long bridge that can take you there. And so, we expect that the distances would be long.\n",
    "\n",
    "Now let's think at the other extreme where we're going to rewire every single one of the edges, and so that would be this network. And so what's happening here is that, we're going to rewire every single edge. And so this network is now completely random. So we've created a bunch of long bridges. And so presumably, distances are pretty small between different nodes. But now, we kind of destroyed the sort of local structure that we had before. And so now we probably don't have many triangles there. And so while the distances are small, now the clustering also got very small.\n",
    "\n",
    "If you're In between, if p is between 0 and 1, then what you have is that some edges get rewire, so you create some long bridges. And so the distances between nodes, the average distance gets reduced. But the local structure depending on p can be maintained. So you can maintain that high clustering as well. So there's a sweet spot for the parameter p, where you can maintain both the short distances as well as the high clustering\n",
    "\n",
    "\n",
    "- What is the Average Clustering Coefficient and Shortest Path of a Small World Network?\n",
    "\n",
    "$\\rightarrow \\text{it depends on the parameters $k$ and $p$}$\n",
    "\n",
    "And so what we see is that, as p increases from 0 to 0.1, notice here that we don't get anywhere close to p = 1. So, this is staying with very small values of p. What we see happening is that, the average shortest path decrease rapidly right after sort of p is away from 0, it just drops down. Whereas, the average clustering coefficient while it also decreases as p increases, it decreases much slower.\n",
    "\n",
    "So for example, an instance of a network with 1,000 nodes and k = 6 and p = 0.04 has the following values. It has a value of 8.99 average shortest path, and 0.53 average clustering coefficient. So for these types of values of p, we can achieve both of the properties that we wanted. The average shortest path being small, single digit, and the average clustering coefficient being pretty large.\n",
    "\n",
    "\n",
    "\n",
    "### Small World Model in NetworkX\n",
    "\n",
    "In `NetworkX`, you can use the function `watts_strogatz_graph(n, k, p)`, with parameters n, k, and p. This returns a Small World Network with `n` nodes, starting with a ring lattice with each node connected to it's `k` nearest neighbors, and with a probability `p` of rewireing the edge. \n",
    "\n",
    "**TIP:** The degree distribution of small world network is not a power law because the degree of most nodes lie in the middle.\n",
    "\n",
    "Now let's wonder about the degree distribution of the small world network. What does it look like? Well, let's use the same code that we used before to visualize the degree distributions of network. This time using a small world network with 1,000 nodes, k = 6, and p = 0.04.\n",
    "\n",
    "Small World Network Degree Distribution:\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graph with 1000 nodes and 4 possible connections from each node\n",
    "G = nx.watts_strogatz_graph(n=1000, k=6, p=0.04)\n",
    "\n",
    "# Estimate the degrees of the network barabasi_albert_graph()\n",
    "degrees = G.degree()\n",
    "\n",
    "# Set of the unique values othe degrees of the network\n",
    "degrees_values = sorted(set(degree.values()))\n",
    "\n",
    "# Histogram\n",
    "histogram = [list(degrees.values()).count(i) / float(nx.number_of_nodes(G)) for i in degrees_values]\n",
    "\n",
    "# Graph - Histogram of the degrees distribution of the network\n",
    "plt.bar(degrees_values, histogram)\n",
    "plt.xlabel(\"Degrees\")\n",
    "plt.ylabel(\"Fraction of Nodes\")\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "So most nodes have degree 6. A few of them have 5 and 7, and I think maybe 1 or various small number of them has degree 4 and 8, and that's about it. And this makes sense because, well, the rewiring probabilities is very small so most of the edges aren't going to be rewired. So most of the nodes are going to stay with their degree of 6 that they had in the beginning when the ring was first formed. And because there's no mechanism that sort of makes some nodes accumulate a very large degree, then none of them do. \n",
    "\n",
    "And so, while this model gets the two properties that we discussed, the clustering coefficient and the shortest paths, it doesn't get the power level degree distribution that we also observe in real networks. And the preferential attachment model gets correctly.\n",
    "\n",
    "\n",
    "Variants of the Small World Model in `NetworkX`:\n",
    "\n",
    "- Small World Networks can be Disconnected, which is sometimes undesirable\n",
    "\n",
    "If you rewire too many of the edges, you could end up with more than one connected component. And this is sometimes something we don't want. Sometimes we want to create a network that is connected, where every node can reach every other node. And so if that's what we need, then we can use the function connected `watts_strogatz_graph(n, k, p, t)`, which has the same parameters except it has an additional parameter `t`.\n",
    "\n",
    "And what it does it runs the standard `watts_strogatz_graph(n, k, p)` model up to `t` times until it returns a connected small world network. So it kind of keeps trying different iterations of the model until it gets one that's connected. Unless it runs out of tries, which if it tries more than t times, then it returns some type of error. \n",
    "\n",
    "There's also the `newman_watts_strogatz_graph(n, k, p)`, which is very similar to its small world model. But instead of rewiring the edges, when it's time to rewire an edge, it actually adds a new edge and leaves the other one still in the network, and still does this with probability p. So instead of rewiring, it adds additional edges.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, today we looked at real networks and we found that they appear to have a small shortest path. And that they tend to have high clustering coefficient. And we found that this preferential attachment model that we talked about last time produces that works that have small shortest paths, but they also have a very small in clustering coefficient. So then we talked about a new model, the small world a model, that starts with a ring lattice with nodes connected to the k nearest neighbors, so it starts out a very high clustering. But then it rewires some of the edges with probability p. And we found that for some values of p, namely for very small values of p, this small world networks have a small average shortest path. Because new long edges are added to that network and those create bridges for nodes to reach each other. And they still maintain these high clustering coefficient, which matches what we observe in the real networks that we look at. However, the degree distribution of a small world network is not a power law, as we had also seen in real data. And on `NetworkX`, you can use the function `watts_strogatz_graph(n, k, p)` or some of the other variants that we covered to produce small world networks.\n",
    "\n",
    "- Real Social Networks appears to have small shortest path between nodes and high clustering coefficient.\n",
    "\n",
    "\n",
    "- The Prefferential Attachment Model produces networks with small Shortest Paths, but with very small Clustering Coefficient.\n",
    "\n",
    "\n",
    "- The Small World Network starts with a ring lattice with nodes connected to $k$ nearest neighbors (*high local clustering coefficient*), and it rewires edges with probability $p$.\n",
    "\n",
    "\n",
    "- For small values of $p$, a Small World Network have small Average Shortest Path and high Clustering Coefficient, matching what we observed in a Real Network.\n",
    "\n",
    "\n",
    "- However, the Degree Distribution of Small World Network is **not a power law**.\n",
    "\n",
    "\n",
    "- On `NetworkX`, you can use the function `watts_strogatz_graph(n, k, p)` or some of the variants that we're covered in order to produce small world networks. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been looking at networks as dynamic structures that change over time. In the past two lessons, we looked at different models of network evolution. That given the dynamics for how a network evolves from the beginning to the end. \n",
    "\n",
    "Now, we're going to look at a related problem which is given a fixed network, **can we predict how this network is going to look in the future**. And more specifically, we're going to be looking at the **link prediction problem**. Which is given a network, can we **predict which edges are going to be formed in the future?** \n",
    "\n",
    "This problem has a lot of applications, for example, if you're Facebook and you want to create a friend recommendation algorithm or a friend recommender to tell people who they should friend. Then, basically you're solving this problem. You're looking at the **current Facebook network and you're trying to predict which new friendships are likely to form in the future**.\n",
    "\n",
    "Let's change the way we first statement\n",
    "- What new edges are more likely to form in the Network?\n",
    "\n",
    "- *Refrase*: Given any Pair of Nodes Which are More likely to connect? \n",
    "$$(node_{i}, node_{j}) \\forall i, \\forall j, \\in Nodes$$\n",
    "\n",
    "\n",
    "Let's say that we take a specific pair of nodes and we want to assess whether or not they're likely to connect. What we're going to do today is we're going to go over several measures, seven of them, to try to make this assessment. To try to decide whether a specific pair of nodes are likely to become friends or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 1 - Common Neighbors\n",
    "\n",
    "\n",
    "If we remember, we talked about *triadic closure* which is the tendency for people who share connections in a social network to become connected themselves. So triadic closure actually gives us a hint for what the first measure that we're going to look at is. And that is very simple measure, just **look at the number of common neighbors that the two nodes have**. Now this is very simple, but let me define this measure to introduce some notation. So we're going to say that **the number of common neighbors of nodes X and Y is going to be the size of a set, which is the intersection of the sets N(X) and N(Y)**, where N(X) defines the set of neighbors of the node X.\n",
    "\n",
    "The numbers of common neighbors of nodes $X$ and $Y$ is:\n",
    "\n",
    "$$\\textit{common neighbors(X, Y)} = |N(X) \\cap N(Y)|, \\textit{where, N(X) defines the set of neighbors of node X}$$\n",
    "\n",
    "\n",
    "In `NetworkX`, we can use the function `common_neighbors()`, which takes in as input the graph and two nodes. And it outputs an iterator of all the common neighbors of the two nodes.\n",
    "\n",
    "What I'm doing is I'm creating a list of tuples which have the two nodes and the number of common neighbors. And I'm only including the nodes that are not connected with each other, the ones that don't have an edge between them by using the function `non_edges()`.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds the number of common neigbors between 2 nodes\n",
    "In: common_neighbors = [(n[0], n[1], len(list(nx.common_neighbors(G, n[0], n[1])))) for n in nx.non_edges(G)]\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (number of common neigbors between nodes)\n",
    "In: common_neighbors_sort = sorted(common_neighbors, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(common_neighbors_sort)\n",
    "Out: [('A', 'C', 2), ('A', 'G', 1), ('A', 'F', 0), ('B', 'I', 3), .... ,('H', 'E' ,0), ('H', 'I', 1)]\n",
    "    \n",
    "# (A, G) = 1\n",
    "# (H, I) = 1\n",
    "```\n",
    "\n",
    "And so if we start to compare between different edges, for example we look at the pair (A, G) and the pair (H, I), and ask, \n",
    "- Which one of these two is more likely to become connected? \n",
    "\n",
    "By looking at the number of common neighbors, **we actually can't tell, because both of these have exactly one neighbor in common**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 2 - Jaccard Coefficient\n",
    "\n",
    "\n",
    "And what this measurement does it **looks at the number of common neighbors, but it normalizes it by the total number of neighbors of the two nodes**. So the way that we write it down is we say the Jaccard coefficient of nodes X and Y is going to be the fraction of the number of common neighbors. That's in the numerator, so it's the intersection of the sets N(X) and N(Y), divided by the number of neighbors of X and Y which would be the union of N(X) and N(Y).\n",
    "\n",
    "The $Jaccard Coefficient$ of nodes $X$ and $Y$ is:\n",
    "\n",
    "$$\\textit{jaccard coefficient(X, Y)} = \\frac{|N(X) \\cap N(Y)|}{|N(X) \\cup N(Y)|} $$\n",
    "\n",
    "\n",
    "In `NetworkX`, we can use the function `jaccard_coefficient()` which takes as input the graph and it outputs an iterator of tuples which have the two nodes and the Jaccard coefficient of the two nodes. But it only outputs the pairs of nodes that are not already connected, so the non edges.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds \"jaccard-probability / coeff\" between 2 nodes of forming a link between them\n",
    "In: possible_neighbors_list = list(nx.jaccard_coefficient(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (jaccard coefficient)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(possible_neighbors_list_sort)\n",
    "Out: [('I', 'H', 1.0), ('A', 'C', 0.553), ('E', 'I', 0.3334), ('E', 'H', 0.2565), .... ,('A', 'H' ,0.0)]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 3 - Research Allocation Index\n",
    "\n",
    "\n",
    "The intuition behind it is that **it considers a fraction of a resource for example, information or something else that a node can send to another node through their common neighbors**. So the Resource Allocation index of the nodes X, Y is going to be **the sum over all the common neighbors of X and Y of one over the degree of the nodes**.\n",
    "\n",
    "$$\\textit{research allocation} = \\sum_{u \\in (N(X) \\cap N(Y))}{ \\frac{1}{|N(u)|} }, \\textit{where, N(u) degrees of node}$$\n",
    "\n",
    "So in this case, if X and Y have a lot of common neighbors and they're going to have a large Resource Allocation index. But if they have a lot of neighbors that have low degree, then they're going to have an even larger Resource \n",
    "Allocation index.\n",
    "\n",
    "- What is the intuition behind Resource Allocation index?\n",
    "\n",
    "Let's consider two nodes X and Y, and let's say that we're measuring the Resource Allocation index between these two nodes. And let's say that they have exactly one common neighbor, Z. Now imagine that X is trying to send to Y a unit of something, let's say information or something else. And is going to do so by passing it for X to Z and then hoping that Z will pass this unit to Y. But actually what Z does is that when it receives this unit from X is going to distribute this unit evenly among all the neighbors of Z. Then in that case, well Y is only going to get a fraction of that unit. And which fraction depends on what the degree of Z is. So if Z has degree N, then Y is only going to get 1 over N of that unit. And so if Z is the only common neighbor of X and Y, and Z has a lot of neighbors, a very large degree. Then X is going to be able to send less to Y, than if Z had a very few neighbors. And so that's why **this research allocation index penalizes pairs of nodes that have common neighbors that themselves have lots of other neighbors**.\n",
    "\n",
    "$$\\textit{Node Z has n neighbors}$$\n",
    "$$\\textit{Node X sents 1 unit of resorces to Node Z}$$\n",
    "$$\\textit{Then, Node Z distributes the 1 unit of resorces evenly, sending $\\frac{1}{n}$ to everyone}$$\n",
    "$$\\rightarrow \\therefore \\textit{Node Y recives $\\frac{1}{n}$ of resources from Node Z}$$\n",
    "\n",
    "\n",
    "We can use the function `resource_allocation_index()` to compute the Resource Allocation index of all pairs of nodes that **are not connected by an edge already in the graph**. And if we sort it, we can see the Resource Allocation index of all pairs in this network.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds resource allocation index between 2 nodes of forming a link between them\n",
    "In: possible_neighbors_list = list(nx.resource_allocation_index(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (resource allocation index)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(possible_neighbors_list_sort)\n",
    "Out: [('A', 'C', 0.6666), ('A', 'H', 0.333), ('E', 'I', 0.2543), ('E', 'H', 0.0565), .... ,('A', 'H' ,0.0)]\n",
    "```\n",
    "\n",
    "\n",
    "### Measurement 4 -  Adamic-Adar Index \n",
    "\n",
    "The next measure is called the Adamic-Adar index. Which is very **similar to the research allocation index, the only difference is that rather than dividing by the degree, it divides by the log of the degree**. \n",
    "\n",
    "$$\\textit{adamic-adar} = \\sum_{u \\in (N(X) \\cap N(Y))}{ \\frac{1}{log(|N(u)|)} }, \\textit{where, N(u) degrees of node}$$\n",
    "\n",
    "There's a function for it too that you can use in `NetworkX` that is `adamic_adar_index()`.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds adamic-adar index between 2 nodes of forming a link between them\n",
    "In: possible_neighbors_list = list(nx.adamic_adar_index(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (adamic-adar index)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 5 -  Preferential Attachment Score\n",
    "\n",
    "\n",
    "So, if we recall, the preferential attachment model has the feature that nodes that have very high degree are more likely to get more neighbors. And so, the intuition behind this measure is that, well, if I'm looking at a pair of nodes and they both have a very high degree, then they're more likely to be connected to each other in the future. \n",
    "\n",
    "And so, what it does, is very simply, to take the product of the degree of the two nodes. Therefore, **the preferential attachment score of X, Y is going to be the product of the number of neighbors of X, times the number of neighbors of Y**.\n",
    "\n",
    "In `NetworkX` we can use the `preferential_attachment()` function to compute the preferential attachment score over all the pairs of nodes that are not connected by an edge already.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds preferential attachment score between 2 nodes\n",
    "In: possible_neighbors_list = list(nx.preferential_attachment(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (preferential attachment score)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(possible_neighbors_list_sort)\n",
    "Out: [('A', 'G', 12), ('C', 'G', 12), ('B', 'G', 10), ('A', 'F', 9), .... ,('I', 'H' , 1)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community Structure -  (Required for the Next two Measurements)\n",
    "\n",
    "Okay, next, we're going to look at two other measures, that take in to account the community structure of the network. So in cases where you have a network and on top of the network, you have some knowledge about different communities.  And here we'll think of communities as sets of nodes and we'll make the assumption that each node belongs only to one community. \n",
    "\n",
    "So one example is if this network was,  of communication among employees in a company. Then you may think that the department for which an employee works for would define a community, such as HR, legal, and so on, and so you could imagine thinking of those as communities. So if you had that type of structure, then you could use different measures for determining whether two nodes are likely to connect to each other or not.\n",
    "\n",
    "$\\Rightarrow$ **So Pair of Nodes who belongs in the same community and have many common neighbors in their community, they are more likely to form an edge**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 6 - Community Common Neighbors / Common Neighbor Soundarajan-Hopcroft Score\n",
    "\n",
    "So the sixth measures **is the number of common neighbors, but with a bonus for neighbors that belong to the same community**. This is called the Common Neighbor Soundarajan-Hopcroft score. \n",
    "\n",
    "And if we're looking at nodes X and Y is simply going to be the number of common neighbors. So the size of intersection of N(X) and N(Y), plus some bonus, which is going to be the sum over all the common neighbors of this function, f(u). And f(u) is simply an indicator that tells us whether the u, which is a common neighbor of X and Y, belongs to the same community as X and Y, or not. And if it does, then it's a 1. If not, it's a 0. So it just simply gives a bonus for the common neighbors that belong to the same community as X and Y.\n",
    "\n",
    "\n",
    "$$\\textit{soundarajan-hopcroft score} = |N(X) \\cap N(Y)| + \\sum_{u \\in (N(X) \\cap N(Y))}{f(u)}$$\n",
    "\n",
    "\\begin{equation}\n",
    "  f(u)=\\begin{cases}\n",
    "    1, & \\text{if $u$ in same common neighbors X, Y}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Now to use this on `NetworkX`, we **first have to tell which communities each node belongs to. So here, we're adding an attribute to each one of the nodes named, community, that tells which community the node belongs to**.  \n",
    "\n",
    "Then after that, now we can use the function `cn_soundarajan_hopcroft()` which outputs an iterator of the tuples with the nodes and the score for each one, each pair, that is not already connected by an edge. And we can sort it and find which nodes have the highest score.\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# Add community attribute to assign each node to a community\n",
    "G.node['A']['community'] = 0\n",
    "G.node['B']['community'] = 0\n",
    "G.node['C']['community'] = 0\n",
    "G.node['D']['community'] = 0\n",
    "G.node['E']['community'] = 1\n",
    "G.node['F']['community'] = 1\n",
    "G.node['G']['community'] = 1\n",
    "G.node['H']['community'] = 1\n",
    "G.node['I']['community'] = 1\n",
    "\n",
    "# List with tuples that holds soundarajan-hopcroft score between 2 nodes\n",
    "In: possible_neighbors_list = list(nx.cn_soundarajan_hopcroft(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (preferential attachment score)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(possible_neighbors_list_sort)\n",
    "Out: [('A', 'C', 4), ('E', 'I', 2), ('B', 'G', 2), ('A', 'F', 1), .... ,('I', 'H' , 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement 7 - Community Research Allocation / Resource Allocation Soundarajan-Hopcroft Score\n",
    "\n",
    "\n",
    "The last measure we're going to look at is a measure that is **similar to the Resource Allocation index but it only takes into account nodes that are in the same community as the two nodes we're looking at**. So if we're computing this measure which is called the Resource Allocation Soundarajan-Hopcroft score. And this is after the researchers that came up with this measure of $X$ and $Y$. Then what we do is we sum over all the neighbors of $X$ and $Y$. And rather than summing just one over the degree of the nodes of the common neighbors like we did in the standard Resource Allocation index. We now have this $f(u)$ in the denominator of the fraction, and this function $f(u)$ again is the same as before, is 1, if $u$ belongs to the same community as $X$ and $Y$, and 0 otherwise. So in the case where you have a common neighbor that does not belong to the same community as $X$ and $Y$, then that neighbor is not contributing anything to the sum because you have a 0 in the numerator.\n",
    "\n",
    "$$\\textit{resource allocation soundarajan-hopcroft score} = \\sum_{u \\in (N(X) \\cap N(Y))}{\\frac{f(u)}{|N(u)|'}}$$\n",
    "\n",
    "\\begin{equation}\n",
    "  f(u)=\\begin{cases}\n",
    "    1, & \\text{if $u$ in same common neighbors X, Y}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We can use this function in `NetworkX` named `ra_index_soundarajan_hopcroft()` to compute the scores of all the non edges.\n",
    "\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "# List with tuples that holds ra_index_soundarajan_hopcroft between 2 nodes\n",
    "In: possible_neighbors_list = list(nx.ra_index_soundarajan_hopcroft(G))\n",
    "\n",
    "# Sort the list by the 3rd element of the tuple (ra_index_soundarajan_hopcroft)\n",
    "In: possible_neighbors_list_sort = sorted(possible_neighbors_list, key=operator.itemgetter(2), reverse=True)\n",
    "\n",
    "# See the results\n",
    "In: print(possible_neighbors_list_sort)\n",
    "Out: [('A', 'C', 0.6666), ('A', 'H', 0.333), ('E', 'I', 0.25), ('E', 'H', 0.25), .... ,('D', 'F' ,0.0)]\n",
    "```\n",
    "\n",
    "$$$$\n",
    "\n",
    "Okay, so these were the seven measures and I want to point out 2 things:\n",
    "\n",
    "- One, none of these measures actually tell you whether or not you should predict that a particular edge is going to come up in the future or not. It just gives a score that is supposed to give you a sense for whether or not these two nodes are likely to connect. \n",
    "\n",
    "\n",
    "- The second thing is that different measures can give you different scores, right? So for example, we saw that some measures would give the edge H, I, a higher score that A,G and some measures would do the opposite. And so these measures aren't necessarily consistent with each other. So if you're actually trying to solve the link-prediction problem, typically what would happen is that you would use these measures as features. And then you would use a classifier, if you have some label data, you would train a classifier and use these measures as features in order to make the prediction.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, we talked about the link prediction problem which says that given a network, we´re tying to predict which edges are going to be formed in the future. And to do so, we've talked about the five different measures, the number of common neighbors, the Jaccard coefficient, which takes the number of common neighbors, but normalizes by the total number of neighbors that the two nodes have. The Resource Allocation index which is thinking along the lines of, if I know I needed to pass information or pass a resource to another node through their common neighbors, how much of that would actually get there? The Adamic-Adar index which is similar to the Resource Allocation index, but it takes the log in the denominator. And the preferential attachment score which bases the score on the idea that in preferential attachment, nodes with high degree tend to accumulate more edges. \n",
    "\n",
    "As well as two measures that require community information. So if you do have community information on your network, then you can use these two, and they're very similar to other ones we saw above. We have one that takes the common neighbors and it gives a bonus for the common neighbors that are in the same community as the two nodes. And then the Resource Allocation one which only considers common neighbors that are in the same community as the two nodes.\n",
    "\n",
    "$$$$\n",
    "\n",
    "$\\hookrightarrow \\texttt{5 Different Measures}$\n",
    "\n",
    "-  **Number of Common Neighbors :** Looks at the number of common neighbors that the two nodes have.\n",
    "\n",
    "\n",
    "- **Jaccard Coefficient :** Looks at the number of common neighbors that the two nodes, but normalizes by the total number of neighbors that the two nodes have.\n",
    "\n",
    "\n",
    "- **Resource Allocation Index :** Considers a fraction of a resource for example, information or something else that a node can send to another node through their common neighbors, is going to be the sum over all the common neighbors of X and Y of one over the degree of the nodes.\n",
    "\n",
    "\n",
    "- **Adamic-Adar Index :** Very similar to the research allocation index, the only difference is that rather than dividing by the degree, it divides by the log of the degree.\n",
    "\n",
    "\n",
    "- **Preferential Attachment Score :** Nodes that have very high degree are more likely to get more neighbors. And so, the intuition behind this measure is that, well, if I'm looking at a pair of nodes and they both have a very high degree, then they're more likely to be connected to each other in the future.\n",
    "\n",
    "$$$$\n",
    "\n",
    "$\\hookrightarrow \\texttt{If Community Structure is present on the Network}$\n",
    "\n",
    "- **Community Common Neighbors / Common Neighbor Soundarajan-Hopcroft Score :** Is the number of common neighbors, but with a bonus for neighbors that belong to the same community.\n",
    "\n",
    "\n",
    "- **Community Research Allocation / Resource Allocation Soundarajan-Hopcroft Score :** Similar to the Resource Allocation index but it only takes into account nodes that are in the same community as the two nodes we're looking at.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
