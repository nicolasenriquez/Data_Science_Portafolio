{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Features from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Texual Features\n",
    "\n",
    "1. Words\n",
    "    - By far the most common class of features\n",
    "    - Stop-Words: Handling or removing the most commonly ocurring words sush as *the*, *is*, *...*\n",
    "    - Normalization decision: Make all the characters in the text lower case vesus leaving them as they are\n",
    "    - Stemming / Lemmatization\n",
    "2. Characteristics of words: Capitalization\n",
    "    - For example, is different to say the White House that to say the white house\n",
    "3. Parts-Of-Speech (POS) in a sentence\n",
    "    - Identify the Nouns, adjectives, pronouns, etc...\n",
    "4. Grouping words of ssimilar meaning / semantics:\n",
    "    - *Example 1*: The words Buy and Purchase refere to the same action\n",
    "    - *Example 2*: Mr., Ms., Dr., Prof., PhD., etc...\n",
    "    - *Example 3*: Numbers/Digits\n",
    "    - *Example 4*: Dates\n",
    "5. Depending on the classification task, features may come from inside words and words sequences\n",
    "    - bigrams, trigrams, n-grams: 'White House'\n",
    "    - character sub-sequences in words: words ending with -'ing', words ending with - 'ion'\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rule\n",
    "\n",
    "- Posterior probability \n",
    "$$\\text{Posterior Probability} = \\frac{\\text{Prior Probability} · \\text{Likelihood}}{\\text{Evidence}}$$\n",
    "\n",
    "$$Pr(\\text{y | X}) = \\frac{P(y)·Pr(\\text{X | y})}{Pr(X)}$$\n",
    "\n",
    "$$y^* = \\underset{y}{\\operatorname{argmax}}  Pr(\\text{y | X}) = \\underset{y}{\\operatorname{argmax}} Pr(y) · \\prod_{i = 1}^{n} Pr(x_{i}\\text{ | }y)$$\n",
    "\n",
    "*Example 1*:\n",
    "**Query** - *Python Download*\n",
    "$$y^* = \\underset{y}{\\operatorname{argmax}} Pr(y) · Pr(\\text{\"Python\" | y}) · Pr(\\text{\"Download\" | y})$$\n",
    "\n",
    "\n",
    "\n",
    "**Naïve Bayes Assumption:** Given the class label, the features are assumed to be independent from each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes: What are the Parameters?\n",
    "\n",
    "What parameters are required in order to be able to apply the Naïve Bayes \n",
    "- **Prior Probabilities**: $Pr(y)$ for all $y$ in $Y$\n",
    "\n",
    "- **Likelihood**: $Pr(x_{i}\\text{ | }y)$ for all features $x_{i}$ and labels $y$ in $Y$\n",
    "\n",
    "- If there are 3 classes $(|Y| = 3)$ and 100 features in $X$, how many parameters does the Naïve Bayes models have?\n",
    "*Answer* = 603 features\n",
    "\n",
    "\n",
    "### Naïve Bayes: Learning Parameters\n",
    "- **Prior Probabilities**: $Pr(y)$ for all $y$ in $Y$\n",
    "    - Count the number of instances in each class\n",
    "    - If there are $N$ instances in all, and $n$ out of those are labeled as class $y$, this implies that, \n",
    "    \n",
    "    $$Pr(y) = \\frac{n}{N}$$\n",
    "    \n",
    "    \n",
    "- **Likelihood**: $Pr(x_{i}|y)$ for all features $x_{i}$ and labels $y$ in $Y$\n",
    "    - Count how many times the feature $x_{i}$ appears in instances labeled as class $y$\n",
    "    - If there are $p$ instances of class $y$, and $x_{i}$ appears in $k$ of those instances, then, \n",
    "    \n",
    "    $$Pr(x_{i}\\text{ | }y) = \\frac{k}{p}$$\n",
    "    \n",
    "\n",
    "### Naïve Bayes: Smoothing\n",
    "- What happends if $Pr(x_{i}\\text{ | }y) = 0$\n",
    "    - Feature $x_{i}$ never occurs in documents labeled $y$\n",
    "    - But then, the posteriori probability $Pr(x_{i}\\text{ | }y)$ will be zero!\n",
    "    \n",
    " To prevent this from happening, instead we apply a smoothing over the parameters\n",
    " - **Laplace Smoothing** or **Additive Smoothing** adds a duumy count to the feature (adds +1)\n",
    " \n",
    " $$Pr(x_{i}\\text{ | }y) = \\frac{(k+1)}{(p+n)}, n=\\text{Number of Features}$$\n",
    "\n",
    "\n",
    "### Naïve Bayes Variations\n",
    "- Multinomial Naïve Bayes\n",
    "    - The data follows a **Multinomial distribution**\n",
    "    - Each feature value represents a count (word ocurrance count, TF-IDF weighting, ...)\n",
    "\n",
    "\n",
    "- Bernouli Naïve Bayes\n",
    "    - The data follow a **Multivariate Bernoulli distribution**\n",
    "    - Each feature is binary (word is present or absent in the text)\n",
    "\n",
    "\n",
    "\n",
    "### Take Home Concepts\n",
    "- Naïve Bayes is a probabilsitic model\n",
    "- Naïve Bayes it assumes that the features are independent from each other, given the class label\n",
    "- For text classification problems, Naïve Bayes models typically provides a very strong baseline model (use first)\n",
    "- It's a very simple model with easy to learn parameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SVM Parameters: Parameter C\n",
    "\n",
    "\n",
    "- Regularization: How much importance should you give to individual data points as compared to better generalized model?\n",
    "\n",
    "\n",
    "- **Regularization C Parameter**\n",
    "    - Larger values of **C = Less Regularization**: Fit training data as well as possible, every data points is important\n",
    "    - Smaller vales of **C = More Regularization**: More tolerant to errors on individual points\n",
    "    \n",
    "\n",
    "### SVM Parameters: Other Parameters\n",
    "\n",
    "- Lineal kernel usually work at it's best for text data\n",
    "    - Other possible kernels: RBF, polynomial\n",
    "\n",
    "- ``multi_class scenario`` One-vs-Rest (ovr): This sets a binary comparison from all the possible classes or labels that are available in the dataset, for example if we have the next 3 classes a comparison would be compare group 3 againt everything else, so this would assign a 1 if the data point belongs to group 3 and 0 otherwise: \n",
    "    1. Group 1: Smokes and white teeth\n",
    "    2. Group 2: Smokes and stained teeth\n",
    "    3. Group 3: Don't Smoke\n",
    "    \n",
    "  A comparison would be compare group 3 againt everything else, so this would **assign a 1 if the data point belongs to group 3 and 0 otherwise**\n",
    "  \n",
    "  \n",
    "  \n",
    "- ``class_weight``: Different classes or labels can get different weights\n",
    "\n",
    "\n",
    "### Take Home Concepts\n",
    "\n",
    "- **Support Vector Machines tend to be the most accurate classifiers, especially in high-dimensional data**\n",
    "\n",
    "\n",
    "- Has strong theoretical foundations (linear algebra, optimization among others)\n",
    "\n",
    "\n",
    "- Can handle **only numeric features**\n",
    "    - Tip 1: Transform all hte categorical features to numeric features\n",
    "    - Tip 2: Normalization of the data (StandardScaler, MinMaxScales, Normalizer, ...)\n",
    "\n",
    "\n",
    "- A draw back of the Support Vector Machine is that the hyperplane that it's used to separate and classify the data it's hard to interpret, so this kind of methods are best to use when it's not so important to know HOW the classification is done, and it's more relevant to have an accurate classifier\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Class 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Text Classifiers in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Text Classification in Natural Languaje Took Kit - NLTK\n",
    "\n",
    "- NLTK has some classification algorithms integrated within the library:\n",
    "\n",
    "    - ```NaiveBayesClassifier```\n",
    "    - ``DecisionTreeClassifier``\n",
    "    - ``ConditionalExponentialClassifier``\n",
    "    - ``MaxentClassifier``\n",
    "    - ``WekaClassifier``\n",
    "    - ``SkleanClassifier``\n",
    "\n",
    "\n",
    "\n",
    "### NaiveBayesClassifier Applied use in Python\n",
    "\n",
    "```python\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "classifier.classify(unlabeled_instance) # binary-class classification\n",
    "classifier.classify_many(unlabeled_instances) # multi-class classification\n",
    "\n",
    "nltk.classify.util.accuracy(classifier, test_set)\n",
    "classifier.labels()\n",
    "\n",
    "classifier.show_most_informative_features()```\n",
    "\n",
    "\n",
    "\n",
    "### SklearnClassifier Applied use in Python\n",
    "\n",
    "```python\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfNB = SklearnClassifier(MultinomialNB()).train(train_set)\n",
    "clfSVC = SklearnClassifier(SVC(), kernel='linear').train(train_set)```\n",
    "\n",
    "\n",
    "### Take Home Concepts\n",
    "\n",
    "- Sklearn is one of the most commonly used for Machine Learning library in Python\n",
    "\n",
    "- NLTK has it's own Naïve Bayes Classifier implementation\n",
    "\n",
    "- NLTK can also interface with Sklearn and other ML toolkits such as Weka\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
