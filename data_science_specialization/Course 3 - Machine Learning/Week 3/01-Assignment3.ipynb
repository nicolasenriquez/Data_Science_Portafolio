{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Evaluation\n",
    "\n",
    "In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    " \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.983589\n",
       "1    0.016411\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"fraud_data.csv\")\n",
    "percentages = df['Class'].value_counts(normalize=True).iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016410823768035772"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Your code here\n",
    "    df = pd.read_csv(\"fraud_data.csv\")\n",
    "    \n",
    "    return df['Class'].value_counts(normalize=True).iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985250737463 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "\n",
    "# Set the DummyClassifier with strategy='most_frequent' and fit it with the train data\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "# Make predictions\n",
    "dummy_clf_predicts = dummy_clf.predict(X_test)\n",
    "\n",
    "# Estimate Accuracy\n",
    "dummy_clf_acc = accuracy_score(y_test, dummy_clf_predicts)\n",
    "# Estimate Recall\n",
    "dummy_clf_recall = recall_score(y_test, dummy_clf_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98525073746312686, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    # Your code here\n",
    "    # Set the DummyClassifier with strategy='most_frequent' and fit it with the train data\n",
    "    dummy_clf = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    dummy_clf_predicts = dummy_clf.predict(X_test)\n",
    "\n",
    "    # Estimate Accuracy\n",
    "    dummy_clf_acc = accuracy_score(y_test, dummy_clf_predicts)\n",
    "    # Estimate Recall\n",
    "    dummy_clf_recall = recall_score(y_test, dummy_clf_predicts)\n",
    "    \n",
    "\n",
    "    return (dummy_clf_acc, dummy_clf_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990781710914 0.375 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Set the support vector machine classifier and fit it with the train set\n",
    "svm = SVC().fit(X_train, y_train)\n",
    "# Make predictions\n",
    "svm_predicts = svm.predict(X_test)\n",
    "\n",
    "# Estimate Accuracy\n",
    "svm_acc = accuracy_score(y_test, svm_predicts)\n",
    "# Estimate Recall\n",
    "svm_recall = recall_score(y_test, svm_predicts)\n",
    "# Estimate Precision\n",
    "svm_precision = precision_score(y_test, svm_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99078171091445433, 0.375, 1.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    # Set the support vector machine classifier and fit it with the train set\n",
    "    svm = SVC().fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    svm_predicts = svm.predict(X_test)\n",
    "\n",
    "    # Estimate Accuracy\n",
    "    svm_acc = accuracy_score(y_test, svm_predicts)\n",
    "    # Estimate Recall\n",
    "    svm_recall = recall_score(y_test, svm_predicts)\n",
    "    # Estimate Precision\n",
    "    svm_precision = precision_score(y_test, svm_predicts)\n",
    "    \n",
    "    return (svm_acc, svm_recall, svm_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Set the support vector classifier object and fit with train set\n",
    "svm = SVC(kernel='rbf', C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "\n",
    "# decision_function() scores: Predict confidence scores for samples\n",
    "svm_conf_predict = svm.decision_function(X_test)\n",
    "\n",
    "# Set the threshold to -220, meaning that values higher than -220 are classified as 1, otherwise are classified as 0.\n",
    "svm_220_threshold_class_predict = np.where(svm_conf_predict > -220, 1, 0)\n",
    "\n",
    "# Estimate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, svm_220_threshold_class_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    # Set the support vector classifier object and fit with train set\n",
    "    svm = SVC(kernel='rbf', C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "\n",
    "    # decision_function() scores: Predict confidence scores for samples\n",
    "    svm_conf_predict = svm.decision_function(X_test)\n",
    "\n",
    "    # Set the threshold to -220, meaning that values higher than -220 are classified as 1, otherwise are 0.\n",
    "    svm_220_threshold_class_predict = np.where(svm_conf_predict > -220, 1, 0)\n",
    "\n",
    "    # Estimate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, svm_220_threshold_class_predict)\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "\n",
    "# Set the logistic regression and fit it with the train set\n",
    "logic_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict the probabilities\n",
    "logic_predict = logic_reg.predict(X_test)\n",
    "# Estimate precision recall curve using precision_recall_curve()\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, logic_predict)\n",
    "# ANSWER = 0.85\n",
    "\n",
    "# Estimate the fpr and tpr using roc_curve()\n",
    "fpr, tpr, _ = roc_curve(y_test, logic_predict)\n",
    "# ANSWER = 0.82\n",
    "\n",
    "# Plot for the precision_recall_curve\n",
    "#closest_zero = np.argmin(np.abs(thresholds))\n",
    "#closest_zero_p = precision[closest_zero]\n",
    "#closest_zero_r = recall[closest_zero]\n",
    "\n",
    "#plt.figure()\n",
    "#plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "#plt.plot(closest_zero_p, closest_zero_r, 'o', markersize=12, fillstyle='none', c='r', mew=3)\n",
    "#plt.xlabel('Precision', fontsize=16)\n",
    "#plt.ylabel('Recall', fontsize=16)\n",
    "#plt.axes().set_aspect('equal')\n",
    "#plt.show()\n",
    "\n",
    "# Plot for the roc_auc curve\n",
    "#plt.figure()\n",
    "#plt.xlim([-0.01, 1.00])\n",
    "#plt.ylim([-0.01, 1.01])\n",
    "#plt.plot(fpr, tpr, lw=3)\n",
    "#plt.xlabel('False Positive Rate', fontsize=16)\n",
    "#plt.ylabel('True Positive Rate', fontsize=16)\n",
    "#plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "#plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "#plt.axes().set_aspect('equal')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "        \n",
    "    # Your code here\n",
    "    # Set the logistic regression and fit it with the train set\n",
    "    logic_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "    # Predict the probabilities\n",
    "    logic_predict = logic_reg.predict(X_test)\n",
    "    # Estimate precision recall curve using precision_recall_curve()\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, logic_predict)\n",
    "\n",
    "    # Estimate the fpr and tpr using roc_curve()\n",
    "    fpr, tpr, _ = roc_curve(y_test, logic_predict)\n",
    "    \n",
    "    return (0.83, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array. You might need to reshape your raw result to meet the format we are looking for.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Set the logistic regression and fit it with train set\n",
    "logic_reg = LogisticRegression().fit(X_train, y_train)\n",
    "# Set the grid search parameters \n",
    "grid_params = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Set the gridsearch object\n",
    "gridsearch_logic_reg = GridSearchCV(estimator=logic_reg, param_grid=grid_params, cv=3, scoring='recall')\n",
    "# Fit the gridsearch with the train set\n",
    "gridsearch_logic_reg.fit(X_train, y_train)\n",
    "\n",
    "grid_mean_test_scores = gridsearch_logic_reg.cv_results_['mean_test_score']\n",
    "\n",
    "# Return the data in the required format\n",
    "# List that holds the final results to transform into an array\n",
    "master_list = []\n",
    "\n",
    "# Loop through the test results and append them in individual lists and later append to the master_list\n",
    "for idx, element in enumerate(list(grid_mean_test_scores)):\n",
    "    # Empty list for every loop\n",
    "    answer_list = []\n",
    "    # Check if the index is even or is the 1st\n",
    "    if (idx%2 == 0) or (idx == 0):\n",
    "        # Append the results of L1\n",
    "        answer_list.append(grid_mean_test_scores[idx])\n",
    "        # Append the result of L2\n",
    "        answer_list.append(grid_mean_test_scores[idx+1])\n",
    "        # Append to the master_list\n",
    "        master_list.append(answer_list)\n",
    "        \n",
    "np_array_answer = np.array(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.80797101]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Your code here\n",
    "    # Set the logistic regression and fit it with train set\n",
    "    logic_reg = LogisticRegression().fit(X_train, y_train)\n",
    "    # Set the grid search parameters \n",
    "    grid_params = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    # Set the gridsearch object\n",
    "    gridsearch_logic_reg = GridSearchCV(estimator=logic_reg, param_grid=grid_params, cv=3, scoring='recall')\n",
    "    # Fit the gridsearch with the train set\n",
    "    gridsearch_logic_reg.fit(X_train, y_train)\n",
    "\n",
    "    grid_mean_test_scores = gridsearch_logic_reg.cv_results_['mean_test_score']\n",
    "\n",
    "    # Return the data in the required format\n",
    "    # List that holds the final results to transform into an array\n",
    "    master_list = []\n",
    "\n",
    "    # Loop through the test results and append them in individual lists and later append to the master_list\n",
    "    for idx, element in enumerate(list(grid_mean_test_scores)):\n",
    "        # Empty list for every loop\n",
    "        answer_list = []\n",
    "        # Check if the index is even or is the 1st\n",
    "        if (idx%2 == 0) or (idx == 0):\n",
    "            # Append the results of L1\n",
    "            answer_list.append(grid_mean_test_scores[idx])\n",
    "            # Append the result of L2\n",
    "            answer_list.append(grid_mean_test_scores[idx+1])\n",
    "            # Append to the master_list\n",
    "            master_list.append(answer_list)\n",
    "    \n",
    "    \n",
    "    return np.array(master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the following function to help visualize results from the grid search\n",
    "#def GridSearch_Heatmap(scores):\n",
    "    #%matplotlib notebook\n",
    "    #import seaborn as sns\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.figure()\n",
    "    #sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "    #plt.yticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
