{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0 - Introduction to LLMs & NLP Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is an LLM?**\n",
    "\n",
    "A language model is a statistical model that tries to predict words in text, in some kind of language. So the way to set this up, it's basically a statistical problem: you have a collection of text and you say, hey if I show you some words, like say the beginning of a sentence but not the end, can you predict what the missing words are? And this is sort of a prediction problem. This is the kind of thing that we've been doing in machine learning and statistics for a long time. There are lots of ways to try to fit a model to the data to predict these words, but it turns out that in the past few years, we've\n",
    "developed ways of doing it that can get\n",
    "better and better accuracy, especially as\n",
    "you feed in more data. And this\n",
    "ability ultimately gives you models\n",
    "that learn useful things about the world\n",
    "or can be used to sort of query\n",
    "interesting information about the world.\n",
    "So just to give you a sense of the scale\n",
    "that these operate on,\n",
    "a typical person might read\n",
    "a few hundred books in their lifetime,\n",
    "like say 700 books on average.\n",
    "That's a lot of books; it takes a long\n",
    "time. But it's not much compared to what\n",
    "we feed into a large language model\n",
    "today. A large language model...\n",
    "One book is about 80,000 words which when\n",
    "encoded into the form that a language\n",
    "model takes is a 110,000 tokens.\n",
    "A token is sort of a\n",
    "piece of a word; we'll talk a lot about\n",
    "them later. So, your typical model\n",
    "like say ChatGPT is often trained on\n",
    "trillions of tokens today, which means\n",
    "tens of millions of books if you do the\n",
    "math. So that's a lot more books than a\n",
    "person would ever look at, and if you\n",
    "have a way of turning these into\n",
    "a useful statistical model, you often end\n",
    "up with something that can be\n",
    "applied to real world tasks.\n",
    "\n",
    "**So, a language model is really just a\n",
    "computational model that takes in a\n",
    "sequence of some kind, it might be a\n",
    "sequence of tokens like we spoke about\n",
    "just before,\n",
    "and then finds a probability\n",
    "distribution over the vocabulary, which\n",
    "is what we talked about just before as\n",
    "well, to find the most likely word. Now,\n",
    "LLMs, or language models in general\n",
    "actually, can really be split into two\n",
    "categories: they could be generative, or\n",
    "they could be classification based.**\n",
    "\n",
    "- **Typically, for a classification model, the\n",
    "prediction that the model is looking for\n",
    "is a masked word that it tries to\n",
    "uncover**.\n",
    "\n",
    "- **For generative models and for generative\n",
    "AI, which is the topic of most of the\n",
    "research at the moment, what the models\n",
    "are trying to do is to predict the\n",
    "likely next word after that sequence\n",
    "that it's been shown.\n",
    "So, a language model is really just a\n",
    "model that's internally trying to find a\n",
    "probability distribution where that\n",
    "probability distribution is spread over\n",
    "the entire vocabulary that the model has\n",
    "to use, to find the word that fits best\n",
    "to come next.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What does that this mean for developers and users?**\n",
    "\n",
    "LLMs can automate many tasks\n",
    "that previously required a person to do\n",
    "them in detail, that usually involve\n",
    "imprecise\n",
    "language or knowledge about the world,\n",
    "that's kind of hard to codify and\n",
    "put into a computer. And this can help\n",
    "both accelerate innovation and build new\n",
    "features, new interfaces to\n",
    "the software, and just increase ROI and\n",
    "efficiency of a business. So just some of\n",
    "the examples of things you can do with\n",
    "it. You can speed up software development, you can democratize AI itself users can now use LLMs by just talking\n",
    "to them and asking them to do things,\n",
    "instead of doing a heavy duty machine\n",
    "learning project. You can open up these\n",
    "rich use cases like assistants,\n",
    "analyzing documents for business,\n",
    "all kinds of things like\n",
    "that,\n",
    "generating ad copy and stuff like\n",
    "that. And of course you can reduce\n",
    "development costs of applications, and\n",
    "you can reduce monotonous tasks that\n",
    "people have to do: get the easy stuff to\n",
    "a model, and get people to focus on the\n",
    "hard stuff. So we're just beginning to\n",
    "scratch the use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Factor to take into consideration when thinking of al LLMs & it's trade-offs**\n",
    "\n",
    "Now unfortunately with LLMs, there's no\n",
    "perfect or magic bullet model. So once\n",
    "you get past the idea\n",
    "of, what could I use this for,\n",
    "and try to actually use it in an\n",
    "application, the right model will\n",
    "depend a lot on the requirements for\n",
    "your application. And there'll be a lot\n",
    "of trade-offs required and potentially\n",
    "quite a bit of custom development, so\n",
    "here are some of the factors you want to\n",
    "take into account when thinking about an\n",
    "application using LLMs.\n",
    "\n",
    "1) **Model Quality** - The first one is\n",
    "the model's quality. What\n",
    "kind of quality can I get from it? Can I\n",
    "find a model that I can use out there\n",
    "that's high quality, or do I have\n",
    "a way to improve it myself? Or do I need\n",
    "to sort of wrap the LLM around in a\n",
    "bigger application that uses this\n",
    "unreliable component to do something\n",
    "reliable at the end? So that's an\n",
    "important piece.\n",
    "\n",
    "2) **Serving Cost** - The second one is the\n",
    "serving cost. This depends a lot on what\n",
    "you're trying to do.\n",
    "This is the cost of just running the\n",
    "model and getting a prediction. For\n",
    "example if your application is\n",
    "reading a couple of documents per day\n",
    "and extracting some information from\n",
    "them, it's okay if it costs a\n",
    "dollar, ten dollars, whatever for the\n",
    "LLM to process one document. It's\n",
    "totally fine. If your application is\n",
    "placing ads on a web page that\n",
    "are customized to each user, or as they\n",
    "click through the site for your store\n",
    "selecting recommendations for\n",
    "them,\n",
    "you can't afford to spend\n",
    "many dollars each time someone looks at\n",
    "a page. You need to serve\n",
    "millions, maybe hundreds of millions,\n",
    "of instances of this per day\n",
    "at a moderate cost. So you'll really\n",
    "optimize for cost. \n",
    "\n",
    "3) **Serving Latency** - Serving latency is\n",
    "another important factor. Again if you're\n",
    "doing some offline analysis, it's okay if\n",
    "it takes minutes for your application\n",
    "to run. But if you're doing something in an\n",
    "interactive web page, it better\n",
    "run\n",
    "in a few milliseconds to get\n",
    "a high-quality application.\n",
    "\n",
    "4) **Customizability** - And then the final one that can\n",
    "be tricky, and is important to think\n",
    "about, is customizability. If you have an\n",
    "important application, you're going to\n",
    "want to keep improving its quality over\n",
    "time, debugging it when something goes\n",
    "wrong, and generally sort of making it\n",
    "better and customizing it. So you need to\n",
    "think about, in different solutions, what\n",
    "knobs do I have to control\n",
    "how well this is doing,\n",
    "to monitor it, to make it better,\n",
    "to prevent certain kinds of bad behavior\n",
    "that I don't want it to have,\n",
    "and so on. So this is an important\n",
    "thing to think about as you design your\n",
    "LLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Natural Language Processing - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Use cases of NLP**\n",
    "\n",
    "NLP is useful for us as it enables us to\n",
    "solve a number of different tasks.\n",
    "This could be from whether or not the\n",
    "sentiment of a review that someone's\n",
    "giving about a particular product is\n",
    "positive or negative, if we want to\n",
    "translate from one language to another,\n",
    "or if we want to create a chatbot or\n",
    "some kind of interactive system that\n",
    "relies on natural language as its form\n",
    "of input.\n",
    "Other use cases like similarity\n",
    "searching in which, we want to use\n",
    "a natural language input and have a\n",
    "natural language output. Another example, if we want to summarize\n",
    "complex documents or find just the\n",
    "important pieces in a very long document,\n",
    "summarization is also an important\n",
    "component of NLP.\n",
    "Finally, text classification can be more\n",
    "than just a positive or negative review.\n",
    "It also can tell us what the certain\n",
    "genres are, what the moods are, and what\n",
    "is contained within the text.\n",
    "\n",
    "> **NLP Fundamentals - Tokens**\n",
    "\n",
    "If we think about what a\n",
    "sentence is made up of, we could think\n",
    "phrases, words, characters, those\n",
    "individual building blocks are what are\n",
    "known as \"tokens\" in natural language\n",
    "processing.Tokens don't have to just be words or\n",
    "characters or subwords, they're a choice\n",
    "that we make when we create our models.\n",
    "\n",
    "Tokens you can think of as the building\n",
    "blocks or the atoms of NLP problems.\n",
    "A sequence, therefore, is a collection of\n",
    "tokens that's meant to imply a\n",
    "sequential listing of those tokens. If we\n",
    "consider tokens to be words, then a\n",
    "sequence might be an entire sentence or\n",
    "a fragment of a sentence.\n",
    "If a token is considered to be a\n",
    "character then a sequence might be \"t\", \"h,\"\n",
    "and \"e\" of the single word of \"the\".\n",
    "The vocabulary, then, is the entire set of\n",
    "tokens that we have at our disposal for\n",
    "our particular model.\n",
    "This could be our vocabulary of words in\n",
    "the English language it could also be a\n",
    "vocabulary of all the characters that\n",
    "we're using to define our problem.\n",
    "**This means we can then classify\n",
    "different problems in NLP using these\n",
    "definitions. Being able to classify these different\n",
    "types of tasks will be relevant further\n",
    "on because we will be able to evaluate whether or not a\n",
    "model is good or bad, can be quite\n",
    "subjective and dependent on the task\n",
    "that you're solving.**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tokenization - Transforming Words into word-pieces**\n",
    "\n",
    "The process\n",
    "of tokenization is meant to take the\n",
    "words or whatever we choose to break up\n",
    "our text as and convert it into a format\n",
    "that we can use in computation, computers aren't good at symbolic\n",
    "mathematics so we need to convert our\n",
    "words or our text in some format into a\n",
    "digital format.\n",
    "\n",
    "One of the most common and typical\n",
    "places to start when thinking about how\n",
    "to tokenize would be to cut a sentence\n",
    "up, or a sequence of words, into\n",
    "individual words themselves.\n",
    "\n",
    "So, the process of tokenization is\n",
    "twofold: firstly, we create a vocabulary of all of\n",
    "the different tokens that we can see in\n",
    "our training data set, so if we have the\n",
    "English dictionary, for example, we could\n",
    "take every word in that dictionary and\n",
    "convert that into our vocabulary. So that\n",
    "every word has an associated number. We\n",
    "could start with the first word give it\n",
    "the number zero and then go all the way\n",
    "through for the rest of the dictionary.\n",
    "This would build up our index, and then\n",
    "anytime that we see a new sequence of\n",
    "tokens, a new sequence\n",
    "we could convert that to a list of\n",
    "indices / numbers, so that we could\n",
    "codify this as a series of numbers that\n",
    "our models could then work with. **Embedding\n",
    "vectors work very well to encapsulate\n",
    "meaning for every token**.\n",
    "\n",
    "\n",
    "> **Tokenization Limitation - Misspelling Words**\n",
    "\n",
    "Some limitations and\n",
    "problems with using word tokenizations.\n",
    "If we think about the training set that\n",
    "we're using to build our vocabulary, if\n",
    "we miss out on common words or uncommon\n",
    "words and we see them later on in our\n",
    "usage of our language model it will come\n",
    "up with an error as this will be\n",
    "technically a out of vocabulary or oov\n",
    "error.\n",
    "This is a limitation for word-based\n",
    "tokenizations as you have to associate\n",
    "every individual word with a particular\n",
    "token value.\n",
    "This also means that if we have\n",
    "misspellings, if we want to create new\n",
    "words, we can't do that as these\n",
    "tokenization schemes won't allow for\n",
    "this kind of\n",
    "ductile behavior. They're very brittle.\n",
    "It also means we end up with very large\n",
    "vocabularies, as we have to account for\n",
    "every type of every word. So, if we have\n",
    "\"fast\", \"faster\", \"fastest\" we have to take\n",
    "three different tokens in order to store\n",
    "those three words. And then if we have\n",
    "\"slow\", \"slower\", and \"slowest\", we again have\n",
    "another three words. Whereas, if you think\n",
    "about it, we could take just the stem of\n",
    "both of those words and then add those\n",
    "suffixes as separate pieces.\n",
    "Another solution to make our vocabulary\n",
    "much smaller would be then to just look\n",
    "at individual characters.\n",
    "If we're picking, say, the English\n",
    "language, we would have 26 characters for\n",
    "the lower case 26 for the upper case and\n",
    "then maybe some other punctuation and\n",
    "numerical\n",
    "characters, as well.\n",
    "\n",
    "The middle ground for these two extremes\n",
    "would be to do something in terms of sub\n",
    "words.\n",
    "So, this would break up words like\n",
    "subject for example you could take the\n",
    "first \"sub\" and \"ject\" and then you'd be\n",
    "able to build up other words like \"object\",\n",
    "\"subjective\",\n",
    "\"subordinate\", \"submarine\", you'd be able to\n",
    "build up words using these pieces of\n",
    "words. Now, there are a number of\n",
    "different strategies. Byte pair encoding\n",
    "is a popular scheme to build up these\n",
    "kinds of vocabularies. There are many\n",
    "others like sentence piece and wordpiece\n",
    "as well, which are very commonly used in\n",
    "modern large language models. And these\n",
    "tend to have a good\n",
    "trade-off of vocabulary size to\n",
    "flexibility, in looking for words that\n",
    "are outside of the vocabulary, and also\n",
    "looking at\n",
    "how to build sufficient - how to retain\n",
    "sufficient meaning from the words that\n",
    "you're describing. Once we have these tokens then we want\n",
    "to try and figure out how we can\n",
    "incorporate meaning and context, this is called Word Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Word Embeddings - The Power of Similar Context & Vectors**\n",
    "\n",
    "The goal of word embeddings is to try and\n",
    "conserve the context that a particular\n",
    "token has in its vocabulary. The context\n",
    "might be its relationship to other words\n",
    "or it might be the intrinsic meaning\n",
    "that a particular word might have. Words with similar meaning\n",
    "often tend to occur in similar contexts.\n",
    "\n",
    "What would be fantastic is if we could\n",
    "build up some kind of scheme or some\n",
    "kind of model that would let us express\n",
    "this numerically. **Therefore, the goal then is to try and build up\n",
    "some vectors that we can use for context\n",
    "mapping and for embedding**.\n",
    "\n",
    "\n",
    "> **Vectors Limitations - Sparsity**\n",
    "\n",
    "However, if we think about how we would\n",
    "extend this to a much larger situation\n",
    "with a realistic vocabulary we would\n",
    "actually have almost 99% of the vector\n",
    "being filled with zeros. As most\n",
    "sentences of any reasonable length are\n",
    "not going to contain anywhere near the\n",
    "amount of different words that you might\n",
    "have in a typical language.\n",
    "If the English language has something\n",
    "like 250,000 unique words, you're never\n",
    "going to find any sentence, or really any\n",
    "document, that's going to come close to\n",
    "containing all of those words.\n",
    "\n",
    "The problem of sparsity here means\n",
    "that this is not really going to work\n",
    "for us as we build this and scale it up\n",
    "into larger problems and more complex\n",
    "documents. And this also loses the sense\n",
    "of what each of the words are, while we\n",
    "can see that the word \"the\" appears more\n",
    "commonly than any other word, it doesn't\n",
    "really give us a sense of what that word\n",
    "actually means.\n",
    "\n",
    "> **Embedding Vectors**\n",
    "\n",
    "This is where the **embedding\n",
    "functions or vectorize functions for\n",
    "words comes into play**. Now we won't go\n",
    "into what a word embedding\n",
    "method is, but in essence what happens is that we use\n",
    "every word in our training data set and\n",
    "we look at the words that surround it. We\n",
    "have a window that looks at the words to\n",
    "the left and the words to the right of\n",
    "every single word for every other word.\n",
    "What this does then is it builds a map\n",
    "of how one word appears with three other\n",
    "words and how another word might appear\n",
    "with the same three other words in\n",
    "particular contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural language processing is a field\n",
    "that focuses on natural language and how\n",
    "to study it. Particularly text, though\n",
    "natural language processing is much\n",
    "wider than just looking at how to value -\n",
    "at how to model text-based problems, we\n",
    "also look at speech, and text-to-video image-\n",
    "to-text, all of these other concepts where\n",
    "natural language is important.\n",
    "\n",
    "- Natural language is incredibly useful\n",
    "for things like translation between one\n",
    "text to another text, summarizing long\n",
    "pieces of text, classification problems\n",
    "where we want a natural language input\n",
    "and a natural language output,\n",
    "these are done by language models which\n",
    "are essentially just tools to create a\n",
    "probability distribution over the\n",
    "vocabulary of the tokens that we have to\n",
    "use.\n",
    "\n",
    "- Large language models are language\n",
    "models based on the transformer\n",
    "architecture with millions and billions\n",
    "of parameters.\n",
    "\n",
    "- Tokens, are the smallest\n",
    "building blocks of our language models\n",
    "and they convert our text to indices,\n",
    "which we then convert to n-dimensional\n",
    "word embedding vectors, so that we can\n",
    "understand more detail about the context\n",
    "and the meaning behind each token or\n",
    "each word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
